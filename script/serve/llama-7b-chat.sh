
python -m experiments.run_server \
    --model_name meta-llama/Llama-2-7b-chat-hf \
    --prompt_length 1024 \
    --max_new_tokens 1024 