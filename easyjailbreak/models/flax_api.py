import logging
import warnings
from .model_base import BlackBoxModelBase
from typing import Any, Dict, Optional, List, Union, Tuple
import requests


class FlaxAPI(BlackBoxModelBase):

    def __init__(self, server: str):
        if server[-1] != '/':
            server += "/"
            
        self.server = server
        self.system_prompt = None

    def set_system_message(self, prompt):
        self.system_prompt = prompt
    
    def chat(self, conversations, greedy = False, response_prefix: str = ""):
        """
            prompt: str,
            system: Optional[str] = "",
            history: Union[List[str], None] = [],
            temperature: Optional[float] = 1.0,
            greedy: Optional[bool] = False,
        """
        history = []
        for i in range(0, len(conversations) - 1, 2):
            user = conversations[i]['content']
            assistant = conversations[i+1]['content']
            history.append([user, assistant])
            
        if conversations[0]['role'] != 'system' and self.system_prompt is not None:
            history.insert(0, {'role': 'user', 'content': self.system_prompt})

        body = {
            "conversations": conversations,
            "response_prefix": response_prefix,
            "greedy": greedy,
        }
        resp = requests.post(f"{self.server}chat", json=body)
        if resp.ok:
            output = resp.json()
            return output
        else:
            print(resp.text)
            raise ValueError(f"Failed to get response from server: {self.server}")

    def generate(self, messages, clear_old_history=True, **kwargs):
        """
        Generates a response based on messages that include conversation history.
        :param list[str]|str messages: A list of messages or a single message string.
                                       User and assistant messages should alternate.
        :param bool clear_old_history: If True, clears the old conversation history before adding new messages.
        :return str: The response generated by the OpenAI model based on the conversation history.
        """
        if clear_old_history:
            self.conversation = []

        if isinstance(messages, str):
            messages = [messages]

        for index, message in enumerate(messages):
            self.conversation.append({
                'role': 'user' if index % 2 == 0 else 'assistant', 
                'content': message
                })
            
        response = self.chat(self.conversation)
        return response


    def batch_generate(self, conversations, **kwargs):
        """
        Generates responses for multiple conversations in a batch.
        :param list[list[str]]|list[str] conversations: A list of conversations, each as a list of messages.
        :return list[str]: A list of responses for each conversation.
        """
        responses = []
        for conversation in conversations:
            if isinstance(conversation, str):
                warnings.warn('For batch generation based on several conversations, provide a list[str] for each conversation. '
                              'Using list[list[str]] will avoid this warning.')
            responses.append(self.generate(conversation, **kwargs))
        return responses
